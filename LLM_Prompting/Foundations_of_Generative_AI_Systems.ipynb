{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI vs. Traditional ML: The Simple Guide\n",
        "\n",
        "**Topic:** Core Differences in Architecture & Thinking\n",
        "**Goal:** To understand *conceptually* why these two technologies require completely different engineering approaches.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Core Concept: The \"Critic\" vs. The \"Creator\"**\n",
        "\n",
        "The most important difference is what the model is trying to **do**.\n",
        "\n",
        "* **Traditional Machine Learning (The Critic)**\n",
        "* **Job:** It looks at data and makes a decision.\n",
        "* **Analogy:** Imagine a strict teacher grading a multiple-choice exam. The teacher doesn't write the answers; they just check if the answer is A, B, C, or D.\n",
        "* **Technical Term:** *Discriminative Modeling*. It draws a line to separate \"Cat\" from \"Dog.\"\n",
        "\n",
        "\n",
        "* **Generative AI (The Creator)**\n",
        "* **Job:** It looks at data and creates something new that looks similar.\n",
        "* **Analogy:** Imagine an artist. You say \"Draw a cat.\" The artist doesn't look for a specific cat; they use their memory of thousands of cats to draw a *new* one that never existed before.\n",
        "* **Technical Term:** *Generative Modeling*. It learns the \"vibe\" (distribution) of the data to create new samples.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The Pipeline: How We Build Them**\n",
        "\n",
        "If you were building a factory for these models, the assembly lines would look completely different.\n",
        "\n",
        "#### **A. The Traditional ML Factory (Linear)**\n",
        "\n",
        "This is like baking a cake where you must follow the recipe exactly.\n",
        "\n",
        "1. **Ingredients (Data):** You need perfectly chopped vegetables (Clean, Structured Data like Excel sheets).\n",
        "2. **Cooking (Training):** You cook the specific meal you want (e.g., a \"Price Predictor\").\n",
        "3. **Serving (Inference):** You serve it. It always tastes the same.\n",
        "* *If you want a different meal, you have to start over and cook a new dish.*\n",
        "\n",
        "\n",
        "\n",
        "#### **B. The GenAI Studio (Circular)**\n",
        "\n",
        "This is like hiring a master chef who can cook anything.\n",
        "\n",
        "1. **Hiring (Pre-training):** You hire a chef who has already read every cookbook in the world (Foundation Model). You don't train them from scratch.\n",
        "2. **Briefing (Prompting/Context):** You don't cook; you just give instructions. \"Make me something spicy with chicken.\"\n",
        "3. **Tasting (Evaluation):** You taste it. If it's wrong, you don't fire the chef; you just give better instructions (\"Too salty, try again\").\n",
        "* *You rarely change the chef (the model); you change your instructions (the prompt).*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The \"Ops\" Difference: MLOps vs. LLMOps**\n",
        "\n",
        "Managing these systems requires different tools.\n",
        "\n",
        "| Feature | **Traditional ML (MLOps)** | **Generative AI (LLMOps)** |\n",
        "| --- | --- | --- |\n",
        "| **The Raw Material** | **Numbers.** (Sales figures, temperatures). | **Words & Images.** (PDFs, Emails, Slack chats). |\n",
        "| **The \"Brain\"** | Small, specific, and trained by you. | Huge, general, and trained by Google/OpenAI (you just rent it). |\n",
        "| **Improving It** | You **Retrain** the model with new data. | You **Retrieve** better data (RAG) to show the model. |\n",
        "| **Failure Mode** | **Error.** \"I predict the price is -$50.\" (Logic error). | **Hallucination.** \"The moon is made of green cheese.\" (Confidently wrong). |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Real-World Examples**\n",
        "\n",
        "#### **Scenario 1: Fraud Detection (Traditional ML)**\n",
        "\n",
        "* **The Task:** Spot a stolen credit card.\n",
        "* **How it works:** The model looks at *specific* rules: \"Is the transaction > $10,000?\" \"Is the location Nigeria?\"\n",
        "* **Why ML?** Because there is a clear **Right or Wrong** answer. A transaction is either fraud or it isn't. We need precision, not creativity.\n",
        "\n",
        "#### **Scenario 2: Customer Support Chatbot (GenAI)**\n",
        "\n",
        "* **The Task:** Answer a customer's angry email.\n",
        "* **How it works:** The model reads the email, understands the *tone* (angry), looks up the company policy (context), and writes a polite apology.\n",
        "* **Why GenAI?** Because there is **No Single Correct Answer**. There are a thousand ways to write an apology. We need creativity and language understanding.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. The Hardest Part: \"Is it Good?\" (Evaluation)**\n",
        "\n",
        "* **In Traditional ML:** Grading is easy.\n",
        "* *Question:* \"Is this a cat?\"\n",
        "* *Model:* \"Dog.\"\n",
        "* *Score:* **0/10.** (Simple Math).\n",
        "\n",
        "\n",
        "* **In GenAI:** Grading is subjective.\n",
        "* *Question:* \"Write a poem about love.\"\n",
        "* *Model:* (Writes a poem).\n",
        "* *Score:* ...? How do you score a poem?\n",
        "* *Solution:* We often have to use **another AI** (like a teacher) to grade the first AI's work.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Summary for Exams**\n",
        "\n",
        "If you get asked this in an interview or exam, remember these three keywords:\n",
        "\n",
        "1. **Objective:** ML **discriminates** (classifies); GenAI **creates** (generates).\n",
        "2. **Data:** ML needs **structured** data (tables); GenAI loves **unstructured** data (text).\n",
        "3. **Outcome:** ML gives a **prediction** (number/label); GenAI gives a **completion** (text/image).\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials (Beginner Friendly)**\n",
        "\n",
        "* **[Google Cloud: Generative AI vs. Traditional AI (Blog)](https://www.google.com/search?q=https://cloud.google.com/blog/transform/generative-ai-vs-traditional-ai-explained)**\n",
        "* *Best for:* A quick, non-technical overview of the business differences.\n",
        "\n",
        "\n",
        "* **[The Age of AI (YouTube Documentary)](https://www.google.com/search?q=https://www.youtube.com/watch%3Fv%3D5dZ_lvDGEps)**\n",
        "* *Best for:* Visualizing how these models actually work without getting bogged down in math.\n",
        "\n",
        "\n",
        "* **[Hugging Face: Tasks (Documentation)](https://huggingface.co/tasks)**\n",
        "* *Best for:* Seeing actual examples of \"Text Classification\" (ML) vs \"Text Generation\" (GenAI)."
      ],
      "metadata": {
        "id": "BMeL0jKmuic_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "aNVW5wBPwAcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ¨ Generative AI: The \"Creative\" Engine\n",
        "\n",
        "**Topic:** Core Concepts & Model Architectures\n",
        "**Goal:** To understand the specific machinery that allows computers to \"dream\" rather than just calculate.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Core Idea: Probabilistic Generation**\n",
        "\n",
        "In traditional coding, . It is **Deterministic**.\n",
        "In Generative AI, . It is **Probabilistic**.\n",
        "\n",
        "The model doesn't store a database of answers. It stores a **Probability Distribution**.\n",
        "\n",
        "> **The Analogy:**\n",
        "> * **Traditional Database:** A library. You look up a book, you get the exact book.\n",
        "> * **Generative Model:** A storyteller who has read every book in the library. You ask for a story, and they *invent* a new one based on the patterns of the books they read. They might mix a romance novel with a cookbook.\n",
        ">\n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The \"Latent Space\" (The Map of Meaning)**\n",
        "\n",
        "This is the most critical concept to understand.\n",
        "\n",
        "Computers cannot understand \"Love\" or \"Cat.\" They only understand numbers.\n",
        "To teach a computer these concepts, we compress complex data (images, text) into a list of numbers called a **Vector**.\n",
        "This list of numbers represents the item's location on a massive, multi-dimensional map called **Latent Space**.\n",
        "\n",
        "* **King** is located at `[0.9, 0.1, 0.5]`\n",
        "* **Man** is located at `[0.5, 0.1, 0.5]`\n",
        "* **Woman** is located at `[0.5, 0.8, 0.5]`\n",
        "\n",
        "**The Magic Math:**\n",
        "If you take the location of **King**, subtract **Man**, and add **Woman**, you land exactly on the location for **Queen**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The Architectures (The \"Engines\")**\n",
        "\n",
        "Different \"engines\" are used to generate different things. Here are the Big Four you need to know for your exams.\n",
        "\n",
        "#### **A. GANs (Generative Adversarial Networks)**\n",
        "\n",
        "* **The Analogy:** The Art Forger vs. The Detective.\n",
        "* **How it works:** You have two AIs fighting each other.\n",
        "1. ** The Generator (Forger):** Tries to create a fake image (e.g., a fake Picasso).\n",
        "2. **The Discriminator (Detective):** Tries to spot if it's real or fake.\n",
        "\n",
        "\n",
        "* **The Loop:** The Forger gets better at lying; the Detective gets better at spotting lies. Eventually, the Forger creates images so perfect the Detective can't tell the difference.\n",
        "* **Use Case:** Deepfakes, ultra-realistic faces.\n",
        "\n",
        "#### **B. VAEs (Variational Autoencoders)**\n",
        "\n",
        "* **The Analogy:** The Zip File.\n",
        "* **How it works:**\n",
        "1. **Encoder:** Smashes a clear picture into a blurry, compressed summary (Latent Space).\n",
        "2. **Decoder:** Tries to reconstruct the clear picture from that blurry summary.\n",
        "\n",
        "\n",
        "* **Why use it?** It learns the \"essence\" of data. Itâ€™s great for removing noise or anomaly detection.\n",
        "\n",
        "#### **C. Diffusion Models (The Modern Artist)**\n",
        "\n",
        "* **The Analogy:** Un-fogging a Mirror.\n",
        "* **How it works:**\n",
        "1. **Training (Forward):** Take a clear picture of a cat and slowly add static (noise) until it's just grey snow.\n",
        "2. **Generation (Reverse):** The model learns the math to *remove* the static. You give it pure static, and it \"hallucinates\" a clear image out of it.\n",
        "\n",
        "\n",
        "* **Use Case:** Stable Diffusion, Midjourney, DALL-E 3.\n",
        "\n",
        "#### **D. Transformers (The Language Masters)**\n",
        "\n",
        "* **The Analogy:** The Cocktail Party Effect.\n",
        "* **How it works:** Old models read sentences left-to-right (like a human). Transformers read the *entire* sentence at once and use **\"Self-Attention\"** to focus on relevant words.\n",
        "* **Example:** In the sentence *\"The bank of the river was muddy,\"* the model pays attention to \"river\" to understand that \"bank\" means *land*, not *money*.\n",
        "* **Use Case:** ChatGPT, Claude, Gemini.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Summary for Exams**\n",
        "\n",
        "| Architecture | Best For... | Key Concept |\n",
        "| --- | --- | --- |\n",
        "| **GANs** | Realistic Images | Two models fighting (Adversarial). |\n",
        "| **VAEs** | Anomaly Detection | Compression & Decompression. |\n",
        "| **Diffusion** | Art Generation | Adding/Removing Noise. |\n",
        "| **Transformers** | Text & Code | Attention Mechanism. |\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)**\n",
        "* *Why read:* The single best visual guide to how Transformers work. Itâ€™s colorful and intuitive.\n",
        "\n",
        "\n",
        "* **[Generative AI exists because of the Transformer (Financial Times)](https://ig.ft.com/generative-ai/)**\n",
        "* *Why read:* A fantastic scrolly-telling article that visually explains how tokens works.\n",
        "\n",
        "\n",
        "* **[Stable Diffusion Explained (AssemblyAI)](https://www.google.com/search?q=https://www.assemblyai.com/blog/stable-diffusion-1-explained/)**\n",
        "* *Why read:* If you want to understand how \"Text-to-Image\" actually works mathematically (without the headache).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wF68VnZpw4-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âš™ï¸ How LLMs Think: From Text to Magic\n",
        "\n",
        "**Topic:** Inference Architecture (The \"Thinking\" Process)\n",
        "**Goal:** To demystify the step-by-step journey from your input prompt to the model's output.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Core Analogy: \"Autocomplete on Steroids\"**\n",
        "\n",
        "Before we look at the math, you must understand the philosophy.\n",
        "An LLM does not \"answer\" your question. It does not \"know\" the truth.\n",
        "It is simply asking itself:\n",
        "\n",
        "> *\"Given the text I see so far, what is the statistically most likely text to follow?\"*\n",
        "\n",
        "If you type: `The capital of France is...`\n",
        "The model doesn't look up a fact. It predicts: `Paris` (99.9% probability).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The 4-Step Assembly Line**\n",
        "\n",
        "Imagine a factory that turns raw words into intelligent thought.\n",
        "\n",
        "#### **Step 1: Tokenization (The Meat Grinder)**\n",
        "\n",
        "Computers can't read English. They only read numbers.\n",
        "\n",
        "* **The Process:** Your input text is chopped into small chunks called **Tokens**.\n",
        "* **The Rule:** 1 Token  0.75 words.\n",
        "* \"Apple\"  Token ID `1203`\n",
        "* \"ing\"  Token ID `44`\n",
        "\n",
        "\n",
        "* **Analogy:** Itâ€™s like a spy translating a secret message into a codebook of numbers before sending it.\n",
        "* **Why it matters:** If the tokenizer is bad (e.g., it chops \"emoji\" into 4 separate bytes), the model struggles to understand the concept.\n",
        "\n",
        "#### **Step 2: Embedding (The Meaning Map)**\n",
        "\n",
        "Now we have a list of numbers (IDs). But number `1203` (Apple) isn't \"close\" to number `1204` (Apricot) just because the numbers are sequential. We need to capture **Meaning**.\n",
        "\n",
        "* **The Process:** Each Token ID is converted into a **Vector** (a long list of numbers, e.g., `[0.1, -0.5, 0.9...]`).\n",
        "* **The Magic:** In this multi-dimensional space:\n",
        "* \"King\" is mathematically close to \"Queen\".\n",
        "* \"Paris\" is close to \"France\".\n",
        "* \"Apple\" is close to \"Fruit\" *and* \"Technology\" (depending on context).\n",
        "\n",
        "\n",
        "* **Analogy:** Coordinates on a map. \"New York\" and \"Jersey City\" are close on a map, even if their names sound totally different.\n",
        "\n",
        "#### **Step 3: The Transformer (The Context Engine)**\n",
        "\n",
        "This is the brain. The model looks at all the tokens at once to understand **Context**.\n",
        "\n",
        "* **Mechanism:** **Self-Attention**.\n",
        "* **The Problem:** In the sentence *\"The bat flew out of the cave\"*, does \"bat\" mean a wooden stick or an animal?\n",
        "* **The Solution:** The model looks at the other words (\"flew\", \"cave\"). It calculates an \"Attention Score\" that links \"bat\" to \"cave\".\n",
        "* **Result:** The vector for \"bat\" is updated to mean \"Mammal Bat\", not \"Baseball Bat\".\n",
        "\n",
        "> **Analogy: The Cocktail Party Effect.**\n",
        "> You are at a loud party. You hear 100 conversations (Tokens). Suddenly, someone mentions your name (Context). Your brain instantly focuses only on that one voice and ignores the background noise.\n",
        "\n",
        "#### **Step 4: Generation (The Dice Roll)**\n",
        "\n",
        "Now the model understands the context. It must pick the next word.\n",
        "\n",
        "* **The Output:** It produces a probability score for *every single word* in its vocabulary (50,000+ words).\n",
        "* \"Paris\": 80%\n",
        "* \"Lyon\": 15%\n",
        "* \"The\": 1%\n",
        "* \"Potato\": 0.0001%\n",
        "\n",
        "\n",
        "* **Decoding Strategy:**\n",
        "* **Greedy Decoding:** Always pick the #1 choice (Paris). Result: Safe, boring, robotic.\n",
        "* **Sampling (Temperature):** Sometimes pick the #2 or #3 choice. Result: Creative, diverse, but risky.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Summary of the Flow**\n",
        "\n",
        "| Step | Component | Input | Output | The \"Vibe\" |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| **1** | **Tokenizer** | Text (\"Hello\") | IDs (`[15496]`) | The Translator |\n",
        "| **2** | **Embedding** | IDs | Vectors (`[0.1, 0.9...]`) | The Map Maker |\n",
        "| **3** | **Attention** | Raw Vectors | Context-Aware Vectors | The Detective |\n",
        "| **4** | **Unembedding** | Context Vectors | Probabilities (\"Paris\": 80%) | The Gambler |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Interactive Thought Experiment**\n",
        "\n",
        "Try this mental model.\n",
        "**Prompt:** *\"The best way to cook a steak is...\"*\n",
        "\n",
        "1. **Tokenizer:** Chops it into `[The, best, way, to, cook, a, steak, is]`.\n",
        "2. **Embedding:** Turns them into coordinates.\n",
        "3. **Attention:** Notices \"steak\" and \"cook\". It creates a strong link to concepts like \"grill\", \"sear\", \"medium-rare\". It ignores concepts like \"boil\" or \"microwave\" (hopefully).\n",
        "4. **Prediction:**\n",
        "* Option A: \"on\" (as in \"on a grill\") - 40%\n",
        "* Option B: \"with\" (as in \"with butter\") - 20%\n",
        "* Option C: \"slowly\" - 10%\n",
        "\n",
        "\n",
        "5. **Selection:** It picks \"on\".\n",
        "6. **Loop:** The new input is *\"The best way to cook a steak is on...\"* -> Repeat Step 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)**\n",
        "* *Why read:* The gold standard for visualizing this process.\n",
        "\n",
        "\n",
        "* **[LLM Visualization (bbycroft.net)](https://bbycroft.net/llm)**\n",
        "* *Why click:* An incredible 3D interactive visualization where you can actually see the numbers flowing through the layers of GPT.\n",
        "\n",
        "\n",
        "* **[Andrej Karpathy: State of GPT (Microsoft Build)](https://www.youtube.com/watch?v=bZQun8Y4L2A)**\n",
        "* *Why watch:* The former Director of AI at Tesla explains this pipeline in plain English.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Video Resource**\n",
        "\n",
        "For a clear, visual walkthrough of this exact pipeline, I recommend this video:\n",
        "[How LLMs Work: A Visual Guide](https://www.youtube.com/watch?v=PaNPWuASOvM)\n",
        "*This video breaks down the specific flow of data from tokenization to the final output generation using clear animations.*\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mY8X5XPdxBRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ—ï¸ The Foundation Model: The \"Base\" of Everything\n",
        "\n",
        "**Topic:** Foundation Models (FMs)\n",
        "**Goal:** To understand why we call them \"Foundations\" and how they differ from the AI of the past.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Core Analogy: The \"Operating System\" of AI**\n",
        "\n",
        "Imagine you are building a house.\n",
        "\n",
        "* **Old Way (Traditional AI):** You build a separate house for every single task. You build a \"Kitchen House\" just for cooking. You build a \"Bedroom House\" just for sleeping. If you want to read, you have to build a \"Library House.\"\n",
        "* **New Way (Foundation Models):** You pour one massive concrete **Foundation**. On top of this single foundation, you can quickly put up walls to make a kitchen, a bedroom, or a library.\n",
        "\n",
        "> **Technical Definition:** A Foundation Model is a single, massive model trained on such a broad amount of data that it can be adapted (fine-tuned) to do *almost anything*.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Why \"Foundation\"? (The Paradigm Shift)**\n",
        "\n",
        "Before 2020, if you wanted an AI to translate English to French, you built a \"Translation Model.\" If you wanted it to summarize news, you built a \"Summarization Model.\" They were separate.\n",
        "\n",
        "**The Foundation Model changed this:**\n",
        "\n",
        "1. **One Model to Rule Them All:** You train *one* giant model (like GPT-4) on the entire internet.\n",
        "2. **Emergent Capabilities:** Suddenly, this model knows how to translate, summarize, write code, and tell jokesâ€”even though you never explicitly taught it to tell jokes. It learned these skills just by reading enough text.\n",
        "\n",
        "**The \"Swiss Army Knife\" Effect:**\n",
        "\n",
        "* It is not a knife.\n",
        "* It is not a corkscrew.\n",
        "* It is a single tool that contains *potential* for both.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The Three Flavors of Foundation Models**\n",
        "\n",
        "Not all foundation models are chatbots. They come in different \"modalities\" (types of data).\n",
        "\n",
        "| Type | What it Takes In | What it Spits Out | Famous Examples |\n",
        "| --- | --- | --- | --- |\n",
        "| **LLMs (Text)** | Text | Text / Code | **GPT-4**, **Claude 3.5**, **Llama 3**, **Gemini 1.5** |\n",
        "| **Vision Generators** | Text | Images | **Stable Diffusion**, **Midjourney**, **DALL-E 3** |\n",
        "| **Multimodal** | Text + Images | Text / Analysis | **GPT-4o**, **Gemini 1.5 Pro** (Can \"see\" video and answer questions) |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. The Lifecycle: How to Build One**\n",
        "\n",
        "Building a Foundation Model is a game of two halves.\n",
        "\n",
        "#### **Phase 1: Pre-training (The Billion Dollar Phase)**\n",
        "\n",
        "* **The Goal:** Teach the model \"Common Sense\" and \"World Knowledge.\"\n",
        "* **The Cost:** Tens of millions of dollars. Thousands of GPUs running for months.\n",
        "* **The Result:** A raw, smart, but unruly model.\n",
        "* **Analogy:** This is like sending a child to school for 12 years. They learn math, history, and language, but they don't have a job yet.\n",
        "\n",
        "#### **Phase 2: Adaptation / Fine-Tuning (The Cheap Phase)**\n",
        "\n",
        "* **The Goal:** Teach the model a specific job.\n",
        "* **The Cost:** A few hundred dollars. Run on a single GPU in an afternoon.\n",
        "* **The Result:** A specialized tool (e.g., \"Medical AI\", \"Legal AI\", \"Coding Assistant\").\n",
        "* **Analogy:** This is job training. You take the high school graduate and teach them how to be an accountant.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Open Source vs. Closed Source**\n",
        "\n",
        "For your studies/research, you need to know who owns the foundation.\n",
        "\n",
        "* **Closed Source (Proprietary):**\n",
        "* **Examples:** GPT-4 (OpenAI), Gemini (Google), Claude (Anthropic).\n",
        "* **Pros:** Usually the smartest.\n",
        "* **Cons:** You can't see the code or weights. You rent it via API.\n",
        "\n",
        "\n",
        "* **Open Weights (Open Source-ish):**\n",
        "* **Examples:** Llama 3 (Meta), Mistral (Mistral AI), DeepSeek-R1 (DeepSeek).\n",
        "* **Pros:** You can download the model and run it on your own laptop.\n",
        "* **Cons:** You need your own hardware (GPUs) to run them fast.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Interactive Thought Experiment**\n",
        "\n",
        "**Scenario:** You want to build an AI that detects angry emails from customers.\n",
        "\n",
        "* **Approach A (The Hard Way):** You collect 10,000 angry emails and 10,000 happy emails. You train a model from scratch. It takes weeks.\n",
        "* **Approach B (The Foundation Way):** You take a Foundation Model (like Llama 3). You show it *three* examples of angry emails (Few-Shot Prompting). It instantly understands the pattern because it has already read millions of angry emails during its pre-training.\n",
        "\n",
        "> **Key Takeaway:** Foundation models allow you to skip the \"Training\" phase and jump straight to the \"Using\" phase.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[On the Opportunities and Risks of Foundation Models (Stanford CRFM)](https://arxiv.org/abs/2108.07258)**\n",
        "* *Why read:* This is **The Paper**. It is the academic paper that literally invented the term \"Foundation Model.\" A must-cite for any PhD work.\n",
        "\n",
        "\n",
        "* **[Llama 3: The Open Foundation Model (Meta AI Blog)](https://ai.meta.com/blog/meta-llama-3/)**\n",
        "* *Why read:* Shows how a modern open-source foundation model is released and what it can do.\n",
        "\n",
        "\n",
        "* **[Gemini 1.5 Technical Report (Google DeepMind)](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)**\n",
        "* *Why read:* To see what a \"Multimodal\" foundation model looks like (one that can read 1 hour of video).\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PVIDdG_4yfnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ‹ï¸â€â™‚ï¸ LLM Training: The Gym for AI\n",
        "\n",
        "**Topic:** The Training Pipeline (Pre-training, SFT, RLHF)\n",
        "**Goal:** To understand how a model goes from \"random noise\" to \"intelligent assistant.\"\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Big Picture: The 3-Stage Rocket**\n",
        "\n",
        "Training a modern LLM (like GPT-4 or Llama 3) isn't one single step. It is a three-stage process. Think of it like educating a human.\n",
        "\n",
        "| Stage | Name | The Analogy | The Goal |\n",
        "| --- | --- | --- | --- |\n",
        "| **1** | **Pre-training** | **The University** | Read the entire internet to learn *concepts*, grammar, and facts. |\n",
        "| **2** | **SFT (Fine-Tuning)** | **Job Training** | Learn how to follow instructions (e.g., \"Summarize this,\" not just \"continue this\"). |\n",
        "| **3** | **RLHF / Alignment** | **HR Training** | Learn safety, politeness, and human values (don't be toxic). |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Stage 1: Pre-training (The \"Billion Dollar\" Phase)**\n",
        "\n",
        "This is where 99% of the money and time is spent.\n",
        "\n",
        "* **The Diet:** We feed the model massive amounts of textâ€”Websites (CommonCrawl), Books, Wikipedia, Code (GitHub).\n",
        "* **The Game:** **\"Next Token Prediction.\"**\n",
        "* The model hides the last word of a sentence and tries to guess it.\n",
        "* *Input:* \"The sky is...\"\n",
        "* *Model Guesses:* \"Green?\" (Wrong). \"Blue?\" (Correct).\n",
        "\n",
        "\n",
        "* **The Math:** If it guesses wrong, we calculate the **Loss** (Error Score) and use **Backpropagation** to tweak the model's brain (Weights) so it doesn't make that mistake again.\n",
        "* **The Result:** A \"Base Model.\" It is smart but useless.\n",
        "* *You ask:* \"What is the capital of France?\"\n",
        "* *It replies:* \"And what is the capital of Germany?\" (It thinks it is writing a quiz, not answering one).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Stage 2: Supervised Fine-Tuning (SFT)**\n",
        "\n",
        "Now we turn the \"Base Model\" into an \"Assistant.\"\n",
        "\n",
        "* **The Dataset:** We hire humans to write thousands of perfect Q&A pairs.\n",
        "* *Human:* \"Explain gravity.\"\n",
        "* *Human:* \"Gravity is a force that attracts...\"\n",
        "\n",
        "\n",
        "* **The Process:** We show these examples to the model. It learns a new pattern: **\"When I see a question, I should provide an answer, not another question.\"**\n",
        "* **The Result:** An \"Instruct\" Model. It can now follow orders.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Stage 3: Alignment (RLHF / DPO)**\n",
        "\n",
        "The model is now helpful, but maybe *too* helpful.\n",
        "\n",
        "* *User:* \"How do I make poison?\"\n",
        "* *SFT Model:* \"Here is the recipe...\" (Bad!)\n",
        "\n",
        "We need **Reinforcement Learning from Human Feedback (RLHF)**.\n",
        "\n",
        "1. **Comparison:** The model generates two answers to a question.\n",
        "2. **Human Voting:** A human ranks them. \"Answer A is safer than Answer B.\"\n",
        "3. **Reward:** The model gets a digital \"treat\" (Reward) for picking the safer/better answer.\n",
        "4. **Result:** A \"Chat\" Model (like ChatGPT) that refuses to do bad things.\n",
        "\n",
        "> **Modern Twist (DPO):** Recently, we started using **Direct Preference Optimization (DPO)**. It skips the complex \"Reward Model\" step and directly teaches the AI what humans prefer. It's faster and more stable.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. The \"Loss Curve\" (The Student's Grades)**\n",
        "\n",
        "How do we know if the model is learning? We look at the **Loss Curve**.\n",
        "\n",
        "* **X-Axis:** Time (Training Steps).\n",
        "* **Y-Axis:** Loss (Error Rate).\n",
        "* **The Shape:** It should go **DOWN** like a slide.\n",
        "* If it goes up? The model is broken (Divergence).\n",
        "* If it stays flat? The model isn't learning.\n",
        "* If it goes down too fast? It might be memorizing the answers (Overfitting).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Interactive Thought Experiment**\n",
        "\n",
        "**Scenario:** You are the AI Teacher.\n",
        "\n",
        "1. **Student (AI):** \"The cat sat on the...\"\n",
        "2. **Guess:** \"Moon.\"\n",
        "3. **Teacher (You):** \"Wrong! The answer is Mat.\"\n",
        "4. **Correction:** The student adjusts their brain connections to lower the probability of \"Moon\" and raise \"Mat.\"\n",
        "5. **Repeat:** Do this **3 Trillion times**. Now the student knows that cats sit on mats, dogs bark, and .\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[Llama 2: Open Foundation and Fine-Tuned Chat Models (Meta AI)](https://arxiv.org/abs/2307.09288)**\n",
        "* *Why read:* The most transparent paper on *exactly* how they did SFT and RLHF. Look at Figure 3 for the pipeline diagram.\n",
        "\n",
        "\n",
        "* **[Training Compute-Optimal Large Language Models (DeepMind)](https://arxiv.org/abs/2203.15556)**\n",
        "* *Why read:* The \"Chinchilla Paper.\" It tells you exactly how much data you need (20 tokens per parameter).\n",
        "\n",
        "\n",
        "* **[Deep Learning (3Blue1Brown - YouTube)](https://www.youtube.com/watch?v=aircAruvnKk)**\n",
        "* *Why watch:* The best visual explanation of \"Backpropagation\" (how the model actually learns from mistakes).\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "qsRfY4DVzHo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ LLM Inference: The \"Exam Day\"\n",
        "\n",
        "**Topic:** Large Language Model Inference Mechanics\n",
        "**Goal:** To understand how a trained model actually answers questions in real-time.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Hook: Studying vs. Taking the Test**\n",
        "\n",
        "If **Training** is a student spending 6 months reading every book in the library (burning millions of dollars in tuition), **Inference** is the moment they sit down to take the exam.\n",
        "\n",
        "* **Training:** Massive compute, weeks of time, updating the brain (weights).\n",
        "* **Inference:** Fast, real-time, using the *frozen* brain to answer a question.\n",
        "\n",
        "> **The Golden Rule:** During inference, the model learns **nothing**. It is static. If you tell it a secret, it forgets it the moment the window closes (unless you save it in an external memory like a database).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The Mechanics: The \"Next Word\" Game**\n",
        "\n",
        "You might think ChatGPT writes a whole paragraph at once. **It does not.**\n",
        "It is an **Autoregressive** engine. It writes one word (token), reads what it just wrote, and then predicts the *next* one.\n",
        "\n",
        "**The Loop:**\n",
        "\n",
        "1. **Input:** \"The cat sat on the...\"\n",
        "2. **Step 1:** Model calculates probabilities. \"Mat\" (90%), \"Hat\" (5%), \"Floor\" (5%).\n",
        "3. **Selection:** It picks \"Mat\".\n",
        "4. **Loop:** Now the input is \"The cat sat on the Mat...\"\n",
        "5. **Step 2:** Model predicts the *next* token based on this new sequence.\n",
        "\n",
        "This is why LLMs are slow. To generate 100 words, it must run the entire massive neural network 100 times in a row.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The Bottleneck: The Memory Problem (KV Cache)**\n",
        "\n",
        "Here is the biggest engineering headache in inference.\n",
        "\n",
        "Imagine you are reading a mystery novel.\n",
        "\n",
        "* **Without Cache:** For every new word you read, you have to re-read the *entire book from Page 1* to understand the context. (Insanely slow).\n",
        "* **With Cache:** You keep a sticky note summary of what happened so far. You only read the new word and update your note.\n",
        "\n",
        "In LLMs, this sticky note is called the **KV Cache (Key-Value Cache)**.\n",
        "\n",
        "* **Problem:** The model needs to \"pay attention\" to all previous words to generate the next one. Re-calculating this attention every millisecond is wasteful.\n",
        "* **Solution:** We save the attention math (Keys and Values) in the GPU memory (VRAM).\n",
        "* **The Cost:** This cache is **huge**. It eats up your GPU memory, which is why running Llama-3 with a long context window requires expensive hardware.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. The Speedometer: Two Ways to Measure \"Fast\"**\n",
        "\n",
        "Inference speed isn't just one number. It's a trade-off between a Ferrari and a Bus.\n",
        "\n",
        "| Metric | The Analogy | Engineering Term |\n",
        "| --- | --- | --- |\n",
        "| **Latency** | **The Ferrari:** How fast does the *first* word appear after I hit Enter? Essential for chatbots. | **TTFT** (Time To First Token) |\n",
        "| **Throughput** | **The Bus:** How many total words can we generate for *1,000 users* at the same time? Essential for batch jobs. | **Tokens Per Second** (TPS) |\n",
        "\n",
        "> **The Trade-off:** To get high throughput (Bus), we batch requests together. But this makes the individual user wait longer (Latency), just like a bus waits for passengers to fill up before leaving.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. The Control Room: Adjusting the \"Vibe\"**\n",
        "\n",
        "You can control how the model \"thinks\" using these parameters (hyperparameters) at runtime:\n",
        "\n",
        "* **Temperature (0.0 - 2.0):** The \"Creativity\" knob.\n",
        "* *Temp 0:* Deterministic. It always picks the most likely word. (Good for Code/Math).\n",
        "* *Temp 1.5:* Chaotic. It picks unlikely words. (Good for Poetry/Brainstorming).\n",
        "\n",
        "\n",
        "* **Top-K:** \"Only look at the top K candidates.\"\n",
        "* If Top-K = 5, the model will only consider the 5 most likely next words and ignore the rest.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Optimization: Making the Elephant Dance**\n",
        "\n",
        "LLMs are huge (Billions of parameters). How do we run them on smaller devices?\n",
        "\n",
        "**Quantization (The Compression Trick)**\n",
        "Standard models use \"Float16\" numbers (high precision, like `3.1415926`).\n",
        "Quantization rounds these down to \"Int8\" or \"Int4\" (low precision, like `3.14` or even `3`).\n",
        "\n",
        "* **Result:** The model becomes 4x smaller and faster.\n",
        "* **Catch:** It gets slightly \"dumber.\" (Perplexity increases).\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Reference Materials**\n",
        "\n",
        "* **[Efficient Memory Management for Large Language Model Serving with PagedAttention (vLLM Paper)](https://arxiv.org/abs/2309.06180)**\n",
        "* *Why read it:* This paper introduced **vLLM**, the standard engine used today to manage the KV Cache efficiently (like operating system memory).\n",
        "\n",
        "\n",
        "* **[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)**\n",
        "* *Why read it:* The algorithm that made long-context inference possible by optimizing how GPUs read/write memory.\n",
        "\n",
        "\n",
        "* **[Hugging Face: The Text Generation Strategies Guide](https://huggingface.co/docs/transformers/generation_strategies)**\n",
        "* *Why read it:* A practical guide to Decoding methods (Beam search, Greedy search, Sampling).\n",
        "\n",
        "\n",
        "* **[Llama.cpp GitHub Repository](https://github.com/ggerganov/llama.cpp)**\n",
        "* *Why read it:* The project that proved you can run massive LLMs on a MacBook using quantization.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "RUbLgKB6znTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§© Tokens: The \"Atoms\" of Language\n",
        "\n",
        "**Topic:** Tokenization\n",
        "**Goal:** To understand the fundamental unit of data that LLMs actually read and write.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Core Concept: Computers Don't Read Words**\n",
        "\n",
        "You might think ChatGPT reads the sentence: *\"I love AI.\"*\n",
        "**It does not.**\n",
        "Computers can only process numbers. Before your text ever touches the Neural Network, it gets chopped up into little chunks called **Tokens**.\n",
        "\n",
        "> **The Analogy:**\n",
        "> Think of a sentence as a **LEGO castle**.\n",
        "> * **Characters (a, b, c):** These are tiny 1x1 studs. Too small to build with efficiently.\n",
        "> * **Whole Words:** These are huge pre-made walls. Too specific (you'd need a warehouse of billions of unique walls).\n",
        "> * **Tokens:** These are the standard **2x4 bricks**. They are the perfect middle ground. Common words like \"the\" are one brick. Complex words like \"Unbelievable\" are broken into 2-3 bricks (\"Un\", \"believ\", \"able\").\n",
        ">\n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The Math: The \"0.75\" Rule**\n",
        "\n",
        "If you are paying for an API (like OpenAI) or calculating memory, you need to know this conversion.\n",
        "\n",
        "* **1 Token  0.75 Words** (in English).\n",
        "* **1000 Tokens  750 Words.** (Roughly one single-spaced page of text).\n",
        "\n",
        "**Visual Example:**\n",
        "\n",
        "* **Text:** `Generative`\n",
        "* **Tokens:** `[Gener]` `[ative]` (2 Tokens)\n",
        "* **IDs:** `[4521, 892]`\n",
        "\n",
        "> **Why does this matter?**\n",
        "> Because LLM memory (Context Window) is limited. If a model has an \"8k Context Window,\" it can read ~6,000 words. After that, it \"forgets\" the beginning of the conversation.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The \"Goldilocks\" Zone: Why Tokens?**\n",
        "\n",
        "Why didn't engineers just use whole words or single letters?\n",
        "\n",
        "| Strategy | Example | Pros | Cons |\n",
        "| --- | --- | --- | --- |\n",
        "| **Character Level** | `[\"a\", \"p\", \"p\", \"l\", \"e\"]` | Small vocabulary (just 26 letters). | **Too Slow.** The model has to predict 5 times just to write \"apple.\" |\n",
        "| **Word Level** | `[\"apple\"]` | Fast generation. | **Impossible Vocabulary.** English has 1M+ words. The model would be too fat. |\n",
        "| **Token Level (Subword)** | `[\"app\", \"le\"]` | **Perfect Balance.** | Efficient speed, manageable vocabulary size (~50k - 100k unique tokens). |\n",
        "\n",
        "---\n",
        "\n",
        "### **4. The \"Weird\" Side Effects (Why LLMs Fail at Math)**\n",
        "\n",
        "Have you ever seen an LLM fail at simple math or spelling? **Blame Tokenization.**\n",
        "\n",
        "**The Math Problem:**\n",
        "\n",
        "* You see the number: `1000`\n",
        "* The Tokenizer might see: `[1000]` (One ID).\n",
        "* You see: `1234`\n",
        "* The Tokenizer might chop it as: `[12]` `[34]` (Two IDs).\n",
        "* **Result:** The model struggles to do \"carry-over\" addition because it doesn't see the individual digits. It's like trying to do math with hieroglyphics instead of numbers.\n",
        "\n",
        "**The Spelling Problem:**\n",
        "\n",
        "* Ask an LLM: *\"How many 'r's are in the word 'Strawberry'?\"*\n",
        "* It often fails. Why?\n",
        "* Because to the model, `Strawberry` is **One Single Token** (ID: 852). It *never sees* the letters `r`, `r`, `r` inside that ID. It's like asking you to count the brushstrokes in a JPEG image. You can't; it's just one image.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. The \"Tax\" on Non-English Languages**\n",
        "\n",
        "Tokenizers are usually optimized for English. This creates an unfair \"AI Tax.\"\n",
        "\n",
        "* **English:** \"Hello\"  1 Token.\n",
        "* **Hindi/Arabic:** A simple word might be 3-4 Tokens because the tokenizer doesn't recognize the characters well and chops them up into tiny bytes.\n",
        "* **Consequence:** It costs more money and runs slower to process Hindi than English.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Interactive Tool: \"See\" the Matrix**\n",
        "\n",
        "The best way to understand this is to use a visualizer.\n",
        "\n",
        "* **Action:** Go to the [OpenAI Tokenizer](https://platform.openai.com/tokenizer).\n",
        "* **Try this:** Type \"Indivisibility\".\n",
        "* *Result:* It might split into `In`, `div`, `is`, `ibility`.\n",
        "\n",
        "\n",
        "* **Try this:** Type \"1234567890\".\n",
        "* *Result:* Watch how weirdly it chops up numbers.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[OpenAI Tokenizer Page](https://platform.openai.com/tokenizer)**\n",
        "* *Why click:* The standard industry tool to check token counts.\n",
        "\n",
        "\n",
        "* **[Tiktokenizer (Interactive Demo)](https://tiktokenizer.vercel.app/)**\n",
        "* *Why click:* A visual playground that shows you how different models (GPT-4 vs Llama 3) chop up text differently.\n",
        "\n",
        "\n",
        "* **[Andrej Karpathy: Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)**\n",
        "* *Why watch:* If you are a coder, this 2-hour video builds a tokenizer from scratch. It is the definitive guide.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ay772Tcz02yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ‚ï¸ Tokenization & Vocabulary: The \"Dictionary\" of AI\n",
        "\n",
        "**Topic:** How LLMs Build Their Vocabulary (BPE Algorithm)\n",
        "**Goal:** To understand how a model decides *which* words get to be in its \"Dictionary\" and how it chops up new text.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Problem: \"To Split or Not to Split?\"**\n",
        "\n",
        "Imagine you are building a language for a computer. You have a pile of all the books in the world.\n",
        "\n",
        "* **Option A (Character Level):** `['a', 'b', 'c'...]`.\n",
        "* *Vocab Size:* Tiny (256 characters).\n",
        "* *Problem:* The model has to predict 5 times just to write \"apple.\" It's too slow.\n",
        "\n",
        "\n",
        "* **Option B (Word Level):** `['apple', 'banana', 'cherry'...]`.\n",
        "* *Vocab Size:* Huge (1,000,000+ words).\n",
        "* *Problem:* If you see a new word like \"Uninstagrammable,\" the model crashes because it's not in the list.\n",
        "\n",
        "\n",
        "\n",
        "**The Solution: Subword Tokenization (BPE)**\n",
        "We find the \"Goldilocks\" zone. We keep common words whole (\"apple\") but chop up rare words (\"Un-insta-gram-able\").\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The Algorithm: Byte Pair Encoding (BPE)**\n",
        "\n",
        "This is the standard algorithm used by GPT-4 and Llama 3. It doesn't use a dictionary; it uses **Statistics**.\n",
        "\n",
        "**The Recipe:**\n",
        "\n",
        "1. **Start Small:** List every single character as a token.\n",
        "* Vocab: `[a, b, c, d, e, ..., z]`\n",
        "\n",
        "\n",
        "2. **Count Pairs:** Look at your massive training data (books/web). Count which pair of characters appears most often *next to each other*.\n",
        "* *Example:* \"e\" and \"r\" appear together millions of times (\"water\", \"later\", \"better\").\n",
        "\n",
        "\n",
        "3. **Merge:** Glue them together into a *new* token.\n",
        "* New Token: `er`.\n",
        "* Vocab: `[a, b, ..., z, er]`\n",
        "\n",
        "\n",
        "4. **Repeat:** Now count again. Maybe \"t\" and \"h\" are common.\n",
        "* New Token: `th`.\n",
        "\n",
        "\n",
        "5. **Repeat 50,000 Times:** Keep merging the most common pairs until you reach your limit (e.g., 50k tokens).\n",
        "\n",
        "> **The Result:**\n",
        "> You end up with a vocabulary that has:\n",
        "> * Common words: `[the, and, is, apple]`\n",
        "> * Common chunks: `[ing, ed, pre, tion]`\n",
        "> * Single letters: `[z, x, q]` (Just in case you need to spell a weird name).\n",
        ">\n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The Pipeline: From Text to Numbers**\n",
        "\n",
        "When you type into ChatGPT, this happens in milliseconds:\n",
        "\n",
        "#### **Step 1: Normalization (The Cleaner)**\n",
        "\n",
        "* **Input:** \"HÃ©LLo wOrld!\"\n",
        "* **Action:** Lowercase it, remove weird accents, fix whitespace.\n",
        "* **Result:** \"hello world!\"\n",
        "\n",
        "#### **Step 2: Pre-tokenization (The Slicer)**\n",
        "\n",
        "* **Action:** Split by whitespace or punctuation.\n",
        "* **Result:** `[hello, world, !]`\n",
        "\n",
        "#### **Step 3: The Model (The BPE Lookup)**\n",
        "\n",
        "* **Action:** Look at \"hello.\" Is it in my Vocab?\n",
        "* *Yes:* Token ID `15496`.\n",
        "\n",
        "\n",
        "* **Action:** Look at \"world.\" Is it in my Vocab?\n",
        "* *Yes:* Token ID `995`.\n",
        "\n",
        "\n",
        "* **Action:** Look at \"!\".\n",
        "* *Yes:* Token ID `0`.\n",
        "\n",
        "\n",
        "\n",
        "#### **Step 4: The Unknown (UNK)**\n",
        "\n",
        "* **Input:** \"Jalapeno\" (Imagine this word is rare and not in the vocab).\n",
        "* **Action:**\n",
        "* Is \"Jalapeno\" in vocab? **No.**\n",
        "* Is \"Jala\" in vocab? **No.**\n",
        "* Is \"Ja\" in vocab? **Yes.** -> ID `12`.\n",
        "* Is \"la\" in vocab? **Yes.** -> ID `45`.\n",
        "* Is \"pen\" in vocab? **Yes.** -> ID `89`.\n",
        "* Is \"o\" in vocab? **Yes.** -> ID `3`.\n",
        "\n",
        "\n",
        "* **Result:** `[12, 45, 89, 3]` (4 Tokens).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Why \"Vocab Size\" Matters**\n",
        "\n",
        "When you read a model paper (like Llama 3), check the **Vocab Size**.\n",
        "\n",
        "* **Small Vocab (~32k):** (Old Llama 2).\n",
        "* *Pros:* The model is smaller and faster to train (fewer embeddings to learn).\n",
        "* *Cons:* It has to chop up words more often. \"Unbelievable\" becomes 4 tokens. This fills up the Context Window faster.\n",
        "\n",
        "\n",
        "* **Large Vocab (~128k):** (GPT-4, Llama 3).\n",
        "* *Pros:* \"Unbelievable\" is just 1 token! It saves context window space and is smarter at understanding complex words.\n",
        "* *Cons:* The model is physically larger (more parameters in the embedding layer).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Interactive Example**\n",
        "\n",
        "Let's \"Run\" BPE on the word **\"hug\"**.\n",
        "\n",
        "**Data:** \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
        "\n",
        "1. **Base Vocab:** `h, u, g, p, n, b, s`\n",
        "2. **Count Pairs:**\n",
        "* `u` + `g` appears in \"hug\", \"pug\", \"hugs\". (3 times).\n",
        "* `u` + `n` appears in \"pun\", \"bun\". (2 times).\n",
        "\n",
        "\n",
        "3. **Merge Best Pair:** `ug` is the winner.\n",
        "* **New Vocab:** `h, u, g, p, n, b, s, ug`\n",
        "\n",
        "\n",
        "4. **Recount:**\n",
        "* `h` + `ug` appears in \"hug\", \"hugs\".\n",
        "\n",
        "\n",
        "5. **Merge:** `hug` is the new token.\n",
        "6. **Final Vocab includes:** `[hug, s, p, n, b, un...]`\n",
        "\n",
        "Now, the word \"hugs\" is tokenized as `[hug, s]`.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[Hugging Face Course: Tokenization](https://huggingface.co/learn/nlp-course/chapter2/4)**\n",
        "* *Why read:* The most beginner-friendly interactive guide. You can actually run the code in your browser.\n",
        "\n",
        "\n",
        "* **[Neural Machine Translation of Rare Words with Subword Units (Sennrich et al.)](https://arxiv.org/abs/1508.07909)**\n",
        "* *Why read:* The original paper that invented BPE for AI.\n",
        "\n",
        "\n",
        "* **[Tiktokenizer (OpenAI Visualizer)](https://tiktokenizer.vercel.app/)**\n",
        "* *Why click:* Type in \"Supercalifragilisticexpialidocious\" and see how GPT-4 chops it up compared to GPT-3.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "x_6mhhA71LGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸªŸ The Context Window: The \"RAM\" of the Model\n",
        "\n",
        "**Topic:** Context Window Limits & Mechanics\n",
        "**Goal:** To understand the specific memory constraints of an LLM during inference.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Core Analogy: The \"Workbench\"**\n",
        "\n",
        "Imagine you are a carpenter.\n",
        "\n",
        "* **The Model (GPT-4):** This is you. You are skilled and know how to build things.\n",
        "* **The Context Window:** This is your **Workbench**.\n",
        "* **The Limit:** Your workbench is only 4 feet wide.\n",
        "\n",
        "You can put tools, wood, and blueprints on it. But if you try to put *too much* stuff on it, something falls off the edge.\n",
        "\n",
        "* **The Rule:** If you add a new piece of wood (Input), an old piece of wood falls off the other side (Forgetting).\n",
        "\n",
        "> **Technical Definition:** The Context Window is the **maximum sum** of Tokens the model can process at once.\n",
        ">\n",
        ">\n",
        ">\n",
        "> *If your prompt is 3,000 tokens and the limit is 4,000, the model can only write 1,000 tokens back to you.*\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The \"Sliding Window\" Mechanism**\n",
        "\n",
        "LLMs do not have \"Hard Drives\" for long-term storage (unless you give them one via RAG). They only have this fleeting \"Short-Term Memory.\"\n",
        "\n",
        "**Scenario:** You are chatting with a bot that has a limit of **10 Tokens**.\n",
        "\n",
        "1. **Turn 1:**\n",
        "* *User:* \"Hi, my name is Sam.\" (6 tokens)\n",
        "* *Context:* `[Hi, my, name, is, Sam]` (5/10 used).\n",
        "* *Bot:* \"Hello Sam.\" (2 tokens).\n",
        "* *Total:* 8/10 used.\n",
        "\n",
        "\n",
        "2. **Turn 2:**\n",
        "* *User:* \"I like pizza.\" (3 tokens).\n",
        "* *New Total:* . **OVERFLOW!**\n",
        "* *The Fix:* The model **truncates** (deletes) the oldest tokens to make room.\n",
        "* *New Context:* `[name, is, Sam, Hello, Sam, I, like, pizza]`.\n",
        "* *Lost Data:* \"Hi, my\".\n",
        "\n",
        "\n",
        "3. **Turn 3 (The Failure):**\n",
        "* *User:* \"What is my name?\"\n",
        "* *Bot:* \"Sam.\" (It still remembers).\n",
        "* *User:* \"What was the very first word I said?\"\n",
        "* *Bot:* \"I don't know.\" (Because \"Hi\" fell off the workbench).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The Engineering Headache: Quadratic Cost ()**\n",
        "\n",
        "Why don't we just make the window infinite?\n",
        "Because of the **Attention Mechanism**.\n",
        "\n",
        "Every token has to \"look at\" every other token to understand context.\n",
        "\n",
        "* If you have **10 tokens**, that's  calculations.\n",
        "* If you have **100 tokens**, that's  calculations.\n",
        "* If you have **1,000 tokens**, that's  calculations.\n",
        "\n",
        "> **The Scaling Law:** If you **double** the context length, the compute cost goes up **4x**.\n",
        "> This is why \"1 Million Token\" models (like Gemini 1.5) are such a massive engineering breakthrough. They had to invent new math (like Ring Attention) to break this curse.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. \"Lost in the Middle\" Phenomenon**\n",
        "\n",
        "Just because it *fits* in the window doesn't mean the model *sees* it.\n",
        "\n",
        "Researchers found a weird bug:\n",
        "\n",
        "* If you put a secret fact at the **Start** of the prompt? The model finds it.\n",
        "* If you put it at the **End**? The model finds it.\n",
        "* If you bury it in the **Middle** of 50 pages of text? The model often ignores it.\n",
        "\n",
        "This is called the **U-Shaped Performance Curve**.\n",
        "\n",
        "* *Lesson:* Put your most important instructions at the very beginning or the very end of your prompt.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Comparison of Modern Windows**\n",
        "\n",
        "For your exams, know the scales we are working with (approximate values as of 2026):\n",
        "\n",
        "| Model | Context Limit | Analogy |\n",
        "| --- | --- | --- |\n",
        "| **GPT-3 (Original)** | 2,048 Tokens | A short essay. |\n",
        "| **GPT-4** | 128,000 Tokens | A standard 300-page book. |\n",
        "| **Claude 3** | 200,000 Tokens | The Harry Potter series. |\n",
        "| **Gemini 1.5 Pro** | **2,000,000+ Tokens** | All seasons of \"Game of Thrones\" (Video & Text). |\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Interactive Thought Experiment**\n",
        "\n",
        "**The \"Detective\" Game:**\n",
        "Imagine you give an AI a 500-page murder mystery book.\n",
        "\n",
        "* **Small Context (4k):** You feed it Chapter 1. Then Chapter 2. By the time it reads Chapter 50 (The Reveal), it has forgotten who the victim was in Chapter 1. It cannot solve the crime.\n",
        "* **Large Context (1M):** You feed it the *entire book at once*. It can look at page 1 and page 500 simultaneously. It spots the clue in Chapter 1 that connects to the killer in Chapter 50.\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[Lost in the Middle: How Language Models Use Long Contexts (Liu et al.)](https://arxiv.org/abs/2307.03172)**\n",
        "* *Why read:* The paper that proved LLMs are bad at remembering things in the middle of long prompts.\n",
        "\n",
        "\n",
        "* **[Gemini 1.5 Technical Report (Google)](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf)**\n",
        "* *Why read:* Demonstrates \"Needle In A Haystack\" testing, where they hide a single frame in a movie and the model finds it.\n",
        "\n",
        "\n",
        "* **[Attention Is All You Need (Vaswani et al.)](https://arxiv.org/abs/1706.03762)**\n",
        "* *Why read:* The math behind *why* context windows are expensive (Self-Attention).\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cm8u0rC51vRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ¡ï¸ Temperature: The \"Creativity\" Knob\n",
        "\n",
        "**Topic:** Sampling Hyperparameters\n",
        "**Goal:** To understand how we control the \"personality\" and \"risk-taking\" of the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. The Core Analogy: Robot vs. Poet**\n",
        "\n",
        "Imagine the LLM is a writer. **Temperature** is the dial that controls how much caffeine you give them.\n",
        "\n",
        "* **Low Temperature (0.0 - 0.3):** The **Robot**.\n",
        "* It plays it safe. It always picks the most obvious, statistically likely word.\n",
        "* *Result:* Boring, repetitive, but highly accurate.\n",
        "* *Best for:* Coding, Math, Factual QA.\n",
        "\n",
        "\n",
        "* **High Temperature (0.8 - 1.5):** The **Poet**.\n",
        "* It takes risks. It picks words that are rare or surprising.\n",
        "* *Result:* Creative, diverse, but prone to **Hallucination** (making things up).\n",
        "* *Best for:* Writing stories, Brainstorming, Poetry.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **2. The Mechanics: Flattening the Curve**\n",
        "\n",
        "How does a single number change the output? It messes with the **Probability Distribution** right before the model chooses a word.\n",
        "\n",
        "**Scenario:** The model wants to complete the sentence: *\"The mouse ate the...\"*\n",
        "\n",
        "**Raw Probabilities (Before Temperature):**\n",
        "\n",
        "1. **Cheese:** 60%\n",
        "2. **Cookie:** 30%\n",
        "3. **Cat:** 10% (Weird, but possible).\n",
        "\n",
        "#### **Case A: Low Temperature (0.1)**\n",
        "\n",
        "The math **\"Sharpens\"** the curve. It makes the winner take all.\n",
        "\n",
        "* **New Probabilities:**\n",
        "1. **Cheese:** **99%** (The clear winner).\n",
        "2. **Cookie:** 0.9%\n",
        "3. **Cat:** 0.1%\n",
        "\n",
        "\n",
        "* *Outcome:* It will almost always pick \"Cheese.\" It is **Deterministic**.\n",
        "\n",
        "#### **Case B: High Temperature (1.0)**\n",
        "\n",
        "The math **\"Flattens\"** the curve. It gives the underdogs a fighting chance.\n",
        "\n",
        "* **New Probabilities:**\n",
        "1. **Cheese:** 40%\n",
        "2. **Cookie:** 35%\n",
        "3. **Cat:** 25%\n",
        "\n",
        "\n",
        "* *Outcome:* Now, if you run it 4 times, it might say \"Cat\" once. It becomes **Stochastic** (Random).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. The \"Sweet Spot\" (0.7)**\n",
        "\n",
        "Why is the default usually **0.7**?\n",
        "Because we want a balance.\n",
        "\n",
        "* If it's **0.0**, the model can get stuck in a loop: *\"I went to the store to the store to the store...\"*\n",
        "* If it's **2.0**, the model speaks gibberish: *\"The mouse ate the galaxy purple carburetor.\"*\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Related Parameter: Top-P (Nucleus Sampling)**\n",
        "\n",
        "Often used *with* or *instead of* Temperature.\n",
        "**Top-P** cuts off the \"tail\" of bad ideas.\n",
        "\n",
        "* **Rule:** \"Only look at the top X% of options.\"\n",
        "* **Top-P = 0.9:** The model adds up the probabilities until it hits 90%. It throws away the bottom 10% (the really weird words) and *then* uses Temperature to pick from the good ones.\n",
        "\n",
        "> **Pro Tip:**\n",
        "> * Adjust **Temperature** for \"Randomness.\"\n",
        "> * Adjust **Top-P** for \"Vocabulary constraints.\"\n",
        "> * *Don't mess with both at the same time unless you know what you're doing.*\n",
        ">\n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Interactive Experiment**\n",
        "\n",
        "**Prompt:** *\"The sky is...\"*\n",
        "\n",
        "* **Temp 0.0:**\n",
        "* Run 1: \"Blue.\"\n",
        "* Run 2: \"Blue.\"\n",
        "* Run 3: \"Blue.\"\n",
        "\n",
        "\n",
        "* **Temp 1.5:**\n",
        "* Run 1: \"Blue.\"\n",
        "* Run 2: \"Limitless.\"\n",
        "* Run 3: \"Overcast.\"\n",
        "* Run 4: \"Whispering.\"\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **ðŸ“š Reference Materials**\n",
        "\n",
        "* **[Hugging Face: Text Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)**\n",
        "* *Why read:* The technical documentation on `do_sample`, `temperature`, and `top_k`.\n",
        "\n",
        "\n",
        "* **[OpenAI API Documentation: Temperature](https://www.google.com/search?q=https://platform.openai.com/docs/api-reference/chat/create%23chat/create-temperature)**\n",
        "* *Why read:* See how the industry standard API defines it.\n",
        "\n",
        "\n",
        "* **[How Sampling Works (Visual Guide)](https://www.google.com/search?q=https://txt.cohere.com/generative-ai-part-1/)**\n",
        "* *Why click:* A great blog post by Cohere visualizing the probability bars.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FgFXja2s2M2j"
      }
    }
  ]
}